#! /usr/bin/env python3

import os, sys, code, traceback
import cmd
import readline
import atexit
from collections import OrderedDict
import functools
from functools import reduce
import numpy as np
import torch
import torch.nn as nn
import torchvision
from torchvision import transforms, datasets
from torchvision.models import *
from torch.nn.functional import softmax
from PIL import Image
from pm_base import ShellBase
from pm_helper_classes import Dataset


model = None
image = None

class Config( object ):
    pass 

class Shell( ShellBase, cmd.Cmd ):
    def __init__( self, config ):
        super().__init__()
        self.config = config
        self.rc_lines = []
        self.dataset = None
        #self.use_rawinput = False
        self.cur_frame = sys._getframe().f_back

        try:
            with open( ".pmdebugrc" ) as rc_file:
                self.rc_lines.extend( rc_file )
        except OSError:
            pass        
        self.exec_rc()

        self.init_history( histfile=".pmdebug_history" )
        atexit.register( self.save_history, histfile=".pmdebug_history" )

        if config.dataset:
            self.dataset = Dataset( config.dataset )

    ##############################################
    # Functions overridden from base class go here
    ##############################################
    def precmd( self, line ):
        if line.find( " " ) > 0:
            args = line.split( " " )
            map( str.strip, args )
            c = "{}_{}".format( args.pop( 0 ), args.pop( 0 ) )
            if ( callable( getattr( self, "do_" + c, None ) ) ):
                line = "{} {}".format( c, " ".join( args ) )        
        return line


    def default( self, line ):
        if line[ :1 ] == '!':
            line  = line[ 1: ]

        is_assign = False
        if line.find( '=' ) > 0:
            var, _ = line.split( '=', maxsplit=1 )
            var = var.strip()
            is_assign = True

        locals = self.cur_frame.f_locals
        globals = self.cur_frame.f_globals
        
        try:
            code = compile( line + "\n", "<stdin>", "single" )
            saved_stdin = sys.stdin
            saved_stdout = sys.stdout
            sys.stdin = self.stdin
            sys.stdout = self.stdout

            try:
                exec( code, globals, locals )
            finally:
                sys.stdin = saved_stdin
                sys.stdout = saved_stdout
        except:
            exec_info = sys.exc_info()[ :2 ]
            self.error( traceback.format_exception_only( *exec_info )[ -1 ].strip() )
        else:
            if is_assign and var and var in self.models:
                self.message( "Resyncing model \"{}\"".format( var ) )
                self.resync_model( var )


    ####################################
    # Decorators
    ####################################
    def supports_compare( func ):    # pylint: disable=no-self-argument
        """We store self.compare on the stack to make the decorator reentrant
        This avoids recursion in case a function that supports this decorator
        is called again inside the decorator
        """

        @functools.wraps( func )
        def wrapper( self, *kargs, **kwargs ):
            compare = self.compare
            self.stack.append( self.compare )
            self.compare = None

            if compare:
                self.fig.set_mode( "dual" )
    
            try:
                if compare == "image":
                    self.do_show_image( *kargs, **kwargs )
                elif compare is not None:
                    func( self, compare )       #pylint: disable=not-callable

                func( self, *kargs, **kwargs )  #pylint: disable=not-callable
            except:
                raise
            finally:
                self.compare = self.stack.pop()
                if compare:
                    self.fig.set_mode( "single" )

        return wrapper


    ####################################
    # All do_* command functions go here
    ####################################
    def do_quit( self, args ):
        """Exits the shell
        """
        self.message( "Exiting shell" )
        self.close()
        raise SystemExit


    def do_summary( self, args ):
        """Prints pytorch model summary
        """
        try:
            for layer in model.layers:
                self.message( "{}  {}".format( layer._get_name(), layer.size() ) )
        except:
            self.error( sys.exc_info()[ 1 ] )


    def do_nparams( self, args ):
        """Print the total number of parameters in a model
        """
        model_info, _  = self.get_info_from_context( args )
        if model_info is None:
            self.message( "Model \"{}\" not found. Please set the model in context first".format( args ) )
            return

        model = model_info.model
        n =  sum( reduce( lambda x, y: x * y, p.size() ) for p in model.parameters())
        print( "{:,}".format( n ) )

    do_nparam = do_nparams


    def do_load_image( self, args ):
        """Load a single image from the path specified
        Usage: load image [ path ]
        """
        global image
        
        image_path = os.path.join( self.config.image_path, args )
        if not os.path.isfile( image_path ):
            self.error( "Image not found")
            return
        self.message( "Loading image {}".format( image_path ) )
        image = Image.open( image_path )
        transform = transforms.Compose( [ transforms.Resize( ( self.image_size, self.image_size ) ),
                                          transforms.ToTensor() ] )
        image = transform( image ).float().unsqueeze( 0 )


    def do_image_next( self, args ):
        """Load the next available image from a dataset:
        Usage: image next
        
        This command operates on a dataset. A dataset must be configued for this 
        command. If there are no more images available to be loaded, it keeps the 
        last available image.
        The "image" global variable points to the loaded image.
        """ 
        global image
        if self.dataset is None:
            self.message( "Please configure a dataset first" )
            return

        self.dataset.next()
        image = self.dataset.load()
        self.fig.imshow( image )


    def do_load_checkpoint( self, args ):
        """Load a checkpoint file into the model:
        Usage: load checkpoint [ filename ]

        If no file is specified, checkpoint_name specified in the
        config file is used"""
        model_info = self.cur_model
        if model_info is None:
            self.error( "No default model set in context." )
            return
        
        model = model_info.model

        if args:
            file = os.path.join( self.config.checkpoint_path, args )
        else:
            file = os.path.join( self.config.checkpoint_path, self.config.checkpoint_name )
        if not os.path.isfile( file ):
            self.error( "Checkpoint file not found" )
            return

        chkpoint = torch.load( file, map_location="cpu" )
        self.message( "Model \"{}\", loading checkpoint: {}".format( model_info.name, file ) )
        
        state_dict = chkpoint[ "model" ]

        try:
            model.load_state_dict( state_dict )
        except RuntimeError:
            new_state_dict = OrderedDict( [ ( k[ 7: ], v ) for k, v in state_dict.items() 
                                                            if k.startswith( "module" ) ] )
            model.load_state_dict( new_state_dict )

    do_load_chkp = do_load_checkpoint
    do_laod_chkp = do_load_checkpoint


    def do_show_image( self, args ):
        """Display an image array:
        Usage: show image [ image_var ]
        
        If an (optional) image_var is specified, it's considered as a global 
        variable referencing an image array and tries to show this image.
        If no args are specified, show the image referenced by the
        global "image" variable.
        """ 
        if not args:
            args = "image"
        img = self.in_place_eval( args )
        if img is None:
            self.error( "Could not display image" )
            return

        if not self.fig.imshow( img ):
            self.error( "Unsupported image type" )
            return

    do_show_img = do_show_image


    @supports_compare
    def do_infer_image( self, args ):
        """Run inference on image:
        Usage: infer image [ model_name ]

        If an optional "model_name" is provided, inference is run
        on that model. The "model_name" must be present in context.
        If no "model_name" is provided, inference is run on the current 
        model set in the context.

        Input image is taken from the global "image" variable.
        """
        model_info, _ = self.get_info_from_context( args )
        if model_info is None:
            return

        img = self.load_from_global( "image" )
        if img is None:
            self.error( "Please load an input image first" )
            return

        net = model_info.model
        out = net( image )
        probs, idxs = softmax( out, dim=1 ).topk( 5, dim=1 )
        self.message( "{}:".format( model_info.name ) )
        for idx, prob in zip( idxs[ 0 ], probs[ 0 ] ):
            self.message( "{:<10}{:4.1f}".format( idx.data, prob.data * 100 ) )

    do_infer = do_infer_image


    @supports_compare
    def do_show_first_layer_weights( self, args ):
        """Display the weight vectors in a layer as a grid:
        Usage: show first layer weights [ model name/tensor ]
        
        If no arguments are given, it displays the first Conv layer weights in the default model
        set in the context
        If an optional model name if provided, it displays the first layer weight for that
        model
        Optionally, a weight tensor can be given as an argument.
        """
        if not args and not self.cur_model:
            self.error( "No default model is set. Please set a model in context first." )
            return
        
        title = "Tensor {}".format( args )
        _weight = None
        if args and args not in self.models:
            _weight = self.in_place_eval( args )
            if _weight is None or not isinstance( _weight, torch.Tensor ):
                self.error( "Can not display \"{}\". Not a model in context, or a tensor.".format( args ) )
                return

            if _weight.dim() is not 4 and _weight.size[ 1 ] not in ( 3, 1 ):
                self.error( "Tensor \"{}\" is incorrect shape for display".format( args ) )
                return

        if _weight is None: 
            model_info = self.models[ args ] if args else self.cur_model
            title = "{} first layer weights".format( model_info.name )
            _, conv = model_info.find_first_instance( type=nn.Conv2d )
            if not conv:
                self.error( "No Conv2d layer found" )
                return
            _weight = conv.weight.detach()
        
        self.show_weights_as_grid( _weight, title )

    do_show_flw = do_show_first_layer_weights


    def do_visual_inspect( self, args ):
        fnum = 0
        row = col = 0
        grid_size = 0
        _weights = None
        _data = None

        def _render( fnum ):
            self.show_weights_as_grid( _weights, cursor=fnum )
            self.fig.imshow( _data[ fnum ] )

        def _evaluate():
            nonlocal fnum
            nonlocal row
            nonlocal col
            nonlocal grid_size
            nonlocal _weights
            nonlocal _data

            fnum = 0
            row = col = 0
            self.cur_model.cur_layer.register_forward_hook()
            _ = self.cur_model.model( image )
            _data = self.cur_model.cur_layer.data().squeeze( 0 )
            _weights = self.cur_model.cur_layer.layer.weight.detach().clone()

            # If the layer has more than 3 output filters, reduce them so they
            # can be displayed
            if _weights.size( 1 ) > 3:
                _weights = torch.mean( _weights, dim=1, keepdim=True )

            grid_size = self.compute_grid_size( _weights.size( 0 ) )
            print( self.cur_model.cur_layer.id, self.cur_model.cur_layer.layer )


        def _keypress( event ):
            nonlocal row
            nonlocal col
            _row = row
            _col = col
            if event.key == "left":
                _col = col - 1 if col > 0 else col
            elif event.key == "right":
                _col = col + 1 if col < grid_size - 1 else col
            elif event.key == "up":
                _row = row - 1 if row > 0 else row
            elif event.key == "down":
                _row = row + 1 if row < grid_size - 1 else row
            elif event.key == "pageup":
                self.cur_model.traverse_updown( dir=-1, type=nn.Conv2d )
                _evaluate()
            elif event.key == "pagedown":
                self.cur_model.traverse_updown( dir=1, type=nn.Conv2d )
                _evaluate()
            elif event.key == 'q' or event.key == 'Q':
                self.fig.stop_event_loop()
                return
            fnum = _row * grid_size + _col
            if fnum < _weights.size( 0 ):
                row, col = _row, _col
                _render( fnum )
        
        if self.cur_model is None:
            self.error( "No current model set in context" )
            return

        image = self.load_from_global( "image" )
        if image is None:
            self.error( "Please load an input image first" )
            return

        if not args or args == "first":
            self.cur_model.set_first_layer()

        _evaluate()

        self.fig.set_mode( "dual" )        
        try:
            _render( fnum )
            self.fig.on_event( "key_press_event", _keypress )
        except:
            raise
        finally:
            self.fig.set_mode( "single" )        


    @supports_compare
    def do_show_activations( self, args ):
        model_info, layer_info = self.get_info_from_context( args )

        if model_info is None:
            return

        img = self.load_from_global( "image" )
        if img is None:
            self.error( "Please load an input image first" )
            return

        if self.data_post_process_fn:
            self.message( "Using processing function {}".format( self.data_post_process_fn.__name__ ) )

        layer_info.register_forward_hook()
        net = model_info.model
        out = net( image )
        self.message( "{} out: {}".format( model_info.name, out.argmax() ) )

        title = "{}{} activations".format( model_info.name, layer_info.id )
        self.display_bargraph( layer_info.data(), title, reduce_fn=self.data_post_process_fn )

    do_show_act = do_show_activations


    def do_activations_next( self, args ):
        global image
        
        if self.dataset is None:
            self.error( "No dataset configured" )
            return

        self.dataset.next()
        image = self.dataset.load()
        self.do_show_activations( args=args )

    do_act_next = do_activations_next


    @supports_compare
    def do_show_heatmap( self, args ):
        model_info, layer_info = self.get_info_from_context( args )
        
        if model_info is None:
            self.message( "Please set a model in context first" )
            return

        img = self.load_from_global( "image" )
        if img is None:
            self.error( "No input image available" )
            return

        id, layer = model_info.find_last_instance( layer=nn.Conv2d )
        layer_info = model_info.get_layer_info( id, layer )
        layer_info.register_forward_hook()

        net = model_info.model
        idx = net( image ).argmax()
        
        _, fc = model_info.find_last_instance( layer=nn.Linear )
        fc_weights = fc.weight[ idx ].data.numpy()

        activations = layer_info.data()[ 0 ].data.numpy()        
        nc, h, w = activations.shape
        
        cam = fc_weights.reshape( 1, nc ).dot( activations.reshape( nc, h * w ) )
        cam = cam.reshape( h, w )
        cam = ( cam - np.min( cam ) ) / np.max( cam )
        cam = Image.fromarray( cam )

        _, _, h, w = img.size()     
        cam = cam.resize( ( h, w ), Image.BICUBIC )
        cam = transforms.ToTensor()( cam )[ 0 ]
        
        msg = "{} guess: {}".format( model_info.name, idx )
        self.message( msg )
        window = self.fig.get_or_create_window()
        window.add_title( msg )
        window.add_image( img )
        window.add_image( cam, cmap="jet", alpha=0.5 )
        window.show()

    do_show_heat = do_show_heatmap


    def do_heatmap_next( self, args ):
        global image
        
        if self.dataset is None:
            self.error( "No dataset configured" )
            return

        self.dataset.next()
        image = self.dataset.load()
        self.do_show_heatmap( args=args )

    do_heat_next = do_heatmap_next


    @supports_compare
    def do_show_weights( self, args ):
        model_info, layer_info = self.get_info_from_context( args )
        if model_info is None:
            return

        id, layer = layer_info.id, layer_info.layer
        self.message( "Current layer is {}: {}".format( id, layer ) )

        if self.data_post_process_fn:
            self.message( "Post processing function is {}".format( self.data_post_process_fn.__name__ ) )

        try:
            data = layer_info.layer.weight.unsqueeze( 0 )
        except:
            self.error( "Current layer has no weights")
        else:
            title = "{} weights".format( layer_info.id )
            self.display_bargraph( data, title, reduce_fn=self.data_post_process_fn )

    do_show_weight = do_show_weights
    do_show_wei = do_show_weights


    @supports_compare
    def do_show_grads( self, args ):
        model_info, layer_info = self.get_info_from_context( args )
        if model_info is None:
            return
    
        id, layer = layer_info.id, layer_info.layer
        self.message( "Current layer is {}: {}".format( id, layer ) )

        if self.data_post_process_fn:
            self.message( "Post processing function is {}".format( self.data_post_process_fn.__name__ ) )

        try:
            data = layer_info.layer.weight.grad.unsqueeze( 0 )
        except:
            self.error( "Current layer has no gradients" )
        else:
            title = "{} gradients".format( layer_info.id )
            self.display_bargraph( data, title, reduce_fn=self.data_post_process_fn )


    def do_set_post_process( self, args ):
        if args == "relu":
            fn = torch.nn.ReLU()
        elif args == "mean":
            fn = torch.mean     # pylint: disable=no-member
        elif args == "max":
            fn = torch.max      # pylint: disable=no-member
        elif args == "none" or args == "None":
            fn = None
        else:
            fn = self.load_from_global( args )
            if not fn:
                self.error( "Could not find function \"{}\"".format( args ) )
                return

        if not fn:
            self.message( "Removing post processing function" )
            self.data_post_process_fn = None
            return

        if fn and not callable( fn ):
            self.error( "Not a valid function" )
            return

        self.data_post_process_fn = fn
        if not hasattr( self.data_post_process_fn, "__name__" ):
            self.data_post_process_fn.__name__ = args
        self.message( "Post process function is {}".format( self.data_post_process_fn.__name__ ) )

    do_set_postp = do_set_post_process


    def do_up( self, args ):
        if not self.cur_model:
            self.error( "Please load a model first" )
            return
        
        if not self.cur_model.up():
            self.message( "Already at top" )
        id, layer = self.cur_model.get_cur_id_layer()
        self.message( "Current layer is {}: {}".format( id, layer ) )


    def do_down( self, args ):
        if not self.cur_model:
            self.error( "Please load a model first" )
            return
        if not self.cur_model.down():
            self.message( "Already at bottom" )
        id, layer = self.cur_model.get_cur_id_layer()
        self.message( "Current layer is {}: {}".format( id, layer ) )


    def do_assign( self, args ):
        args = args.split()

        def _set_name_and_check_in_use( default ):
            var = args[ 1 ] if len( args ) > 1 else default    
            if var in globals():
                self.error( "Variable \"{}\" already in use.".format( var ) )
                self.help( "please \"del {}\" first, if you want to reassign this var".format( var ) )
                return
            else:
                return var

        if not self.cur_model:
            self.error( "Please set a model in context first")
            return
        if not args:
            self.error( "No valid options provided" )
            return

        if args[ 0 ] == "layer":
            var = _set_name_and_check_in_use( "layer" )
            globals()[ var ] = self.cur_model.cur_layer.layer

        elif args[ 0 ] == "weight":
            var = _set_name_and_check_in_use( "weight" )
            globals()[ var ] = self.cur_model.cur_layer.layer.weight.data.detach().clone()

        elif args[ 0 ] == "out" or args[ 0 ] == "outsq":
            var = _set_name_and_check_in_use( "out" )
            self.cur_model.cur_layer.register_forward_hook()
            if image is None:
                globals()[ var ] = None
            else:
                _ = self.cur_model.model( image )
            if args[ 0 ] == "outsq":
                data = self.cur_model.cur_layer.data()
                globals()[ var ] = data.squeeze( 0 ) if data is not None else None
            else:
                globals()[ var ] = self.cur_model.cur_layer.data()
        else:
            self.error( "Invalid comand option \"{}\"".format( args[ 0 ] ) )


    def do_set_context( self, args ):
        model_name = args if args else "model"
        model = self.load_from_global( model_name )
        if model is None:
            self.error( "Could not find a model by name \"{}\"".format( model_name ) )
            return

        if not isinstance( model, nn.Module ):
            self.error( "{} is not a valid model" )
            return

        self.set_model( model_name, model )

        self.message( "Context now is \"{}\"".format( model_name ) )
        self.fig.set_window_title( model_name )
    
    do_set_ctx = do_set_context


    def do_resync( self, args ):
        if not args:
            self.error( "Please provide a model name" )
            return

        if args not in self.models:
            self.error( "Model \"{}\" not in context".format( args ) )
            return

        self.resync_model( args )


    def do_set_class( self, args ):
        if self.dataset is None:
            self.error( "No dataset is configured" )
        idx = int( args ) if args else 0
        if not self.dataset.set_class( idx ):
            self.error( "Could not set class to {}".format( args ) )


    def do_set_compare( self, args ):
        if not args:
            self.compare = None
        elif args not in self.models and args not in ( "image", "flw" ):
            self.error( "Model \"{}\" not in context".format( args ) )
            return 
        else:
            self.compare = args

    do_set_comp = do_set_compare


    def do_set_dataset( self, path ):
        self.dataset = Dataset( path )


    def do_dataset_suffix( self, args ):
        if self.dataset.suffix( args ):
            self.message( "Current dataset is: {}".format( self.dataset.data_path ) )
        else:
            self.error( "Could not change suffix" )
        

    ####################################################
    # Helper functions to debugger functionality go here
    ####################################################
    def exec_rc( self ):
        if not self.rc_lines:
            return
        self.message( "\nReading rc file...", end="" )

        self.stack.append( self.quiet )
        self.quiet = True
        num = 1
        while self.rc_lines:
            line = self.rc_lines.pop( 0 ).strip()
            self.message( "{}: {}".format( num, line ), end="" )
            num += 1
            if not line or "#" in line[ 0 ]:
                self.message()
                continue
            self.onecmd( self.precmd( line ) )
        self.message()
        self.quiet = self.stack.pop()
        self.message( "Done" )


    def init_history( self, histfile ):
        try:
            readline.read_history_file( histfile )
        except FileNotFoundError:
            pass        
        readline.set_history_length( 2000 )
        readline.set_auto_history( True )


    def save_history( self, histfile ):
        self.message( "Saving history" )
        readline.write_history_file( histfile )


    def _cmdloop( self, intro_header ):
        self.message( "Welcome to the debug shell!" )
        while True:
            try:
                self.allow_kbdint = True
                self.cmdloop( intro_header )
                self.allow_kbdint = False
                break
            except KeyboardInterrupt:
                self.message( "\n**Keyboard Interrupt" )
            except ( AttributeError, TypeError, NameError ) as e:
                self.error( e )
                traceback.print_exc()
            except ( RuntimeError, IndexError ):
                traceback.print_exc()

if __name__ == "__main__":
    config = Config()
    shell = Shell( config )
    shell.prompt = '>> '
    shell._cmdloop( "" )